{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca7abeb9",
   "metadata": {},
   "source": [
    "- Intructions on this Repo\n",
    "1. Heap size error run this command line before run jupyter notebook on command line\n",
    "\n",
    "export PYSPARK_SUBMIT_ARGS=' --conf spark.sql.shuffle.partitions=700 --conf spark.default.parallelism=700 --driver-memory 30g --driver-cores 6 --executor-memory 30g --executor-cores 6 pyspark-shell'\n",
    "\n",
    "2. ERROR PythonRunner: Python worker exited unexpectedly (crashed) java.net.SocketException: Connection reset\n",
    "\n",
    "Try to run several times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bedb0a",
   "metadata": {},
   "source": [
    "## Demand Forecasting\n",
    "\n",
    "The objective of this notebook is to illuate how we might leverage the Apache Spark - the efficient distribution of the work required to generate hundreds of thousands or even millions of ML models in a timely manner and FBProphet - popular library for demanding forecasting.\n",
    "\n",
    "NOTE: The original notebook utilized the DataBricks and DataLakes. In this notebook, I will use my Spark SQL and localhost storage to emulate the orginal notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f447bf",
   "metadata": {},
   "source": [
    "### Exploring Data\n",
    "- Source: https://www.kaggle.com/competitions/demand-forecasting-kernels-only/data\n",
    "\n",
    "A relatively simple and clean dataset, given 5 years of store-item sales data(10 stores), and asked to predict 3 months of sales for 50 different items at 10 different stores. \n",
    "\n",
    "What's the best way to deal with seasonality? Should stores be modeled separately, or can you pool them together? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36f3bb",
   "metadata": {},
   "source": [
    "#### Import Data from local to sql database\n",
    "- use pyspark to import data to sql database, to work similarly to DataBricks notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b15375fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3350ad",
   "metadata": {},
   "source": [
    "#### Create SQL databse using Pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af0426b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyspark Version: 3.3.0\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "print(f\"Pyspark Version: {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae0cc70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 23:27:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Create a spark Context class\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "179242f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "from delta import *\n",
    "#we need to config sparksession with delta lake for later use in this repo\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\")\\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e78c9d8",
   "metadata": {},
   "source": [
    "#Create spark session\n",
    "spark = SparkSession.builder.master('local[*]').\\\n",
    "            config('spark.sql.debug.maxToStringFields', '100').\\\n",
    "            appName(\"Time Serries Forecasting\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18d032fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the training data into a dataframe, and convert to datatype accordingly\n",
    "#library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#structure of the training dataset\n",
    "train_schema = StructType([\n",
    "    StructField('date', DateType()),\n",
    "    StructField('store', IntegerType()),\n",
    "    StructField('item', IntegerType()),\n",
    "    StructField('sales', IntegerType())\n",
    "])\n",
    "\n",
    "spdf_data = spark.read.csv('./Data/train.csv', header=True, schema=train_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f39c99e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- store: integer (nullable = true)\n",
      " |-- item: integer (nullable = true)\n",
      " |-- sales: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spdf_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26686d63",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o83.createOrReplaceTempView.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1620)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:67)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:86)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:86)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:85)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:51)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:122)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:93)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:109)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:106)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:295)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:295)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:277)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:118)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:136)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:204)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:249)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:92)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\n\tat org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3887)\n\tat org.apache.spark.sql.Dataset.createOrReplaceTempView(Dataset.scala:3455)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:55)\n\t... 80 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#make the temporary view table\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspdf_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateOrReplaceTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lighthouse/lib/python3.9/site-packages/pyspark/sql/dataframe.py:278\u001b[0m, in \u001b[0;36mDataFrame.createOrReplaceTempView\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateOrReplaceTempView\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;124;03m\"\"\"Creates or replaces a local temporary view with this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    The lifetime of this temporary table is tied to the :class:`SparkSession`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateOrReplaceTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lighthouse/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lighthouse/lib/python3.9/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/lighthouse/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o83.createOrReplaceTempView.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1620)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:67)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:86)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:86)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:85)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:51)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:122)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:93)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:109)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:106)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:295)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:295)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:277)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:118)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:136)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:204)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:249)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:92)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\n\tat org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3887)\n\tat org.apache.spark.sql.Dataset.createOrReplaceTempView(Dataset.scala:3455)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:55)\n\t... 80 more\n"
     ]
    }
   ],
   "source": [
    "#make the temporary view table\n",
    "spdf_data.createOrReplaceTempView('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load sparksql magic to run sql queries in notebook cell\n",
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "SELECT * FROM train;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055da5b5",
   "metadata": {},
   "source": [
    "##### General annual trends and seasonality in unit sales\n",
    "NOTE: DataBricks sql is able to visuallize immediately with sql queries in notebook cells, on Jupyter notebook we will create dataframe and visuallize using python visuallize libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c8e32",
   "metadata": {},
   "source": [
    "##### Plot the annual trending "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b0070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "SELECT year(date) as year,\n",
    "        sum(sales) as sales \n",
    "    FROM train \n",
    "    GROUP BY year(date) \n",
    "    ORDER BY year;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect data\n",
    "query_annual_trending ='\\\n",
    "    SELECT year(date) as year,\\\n",
    "            sum(sales) as sales \\\n",
    "    FROM train \\\n",
    "    GROUP BY year(date) \\\n",
    "    ORDER BY year;'\n",
    "collected_annual_trending = spark.sql(query_annual_trending).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1036db",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_annual_trending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e5a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dataframe for plotting\n",
    "df_annual_trending = pd.DataFrame(collected_annual_trending, columns=['year', 'million_sales'])\n",
    "df_annual_trending['million_sales'] = df_annual_trending['million_sales'].apply(lambda x: x/10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78908bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annual_trending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d4549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(df_annual_trending['year'], df_annual_trending['million_sales'], color='red', marker='o')\n",
    "plt.title(\"Annual trending\", fontsize=13)\n",
    "plt.xlabel('year', fontsize=13)\n",
    "plt.ylabel('million sales', fontsize=13)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159923c",
   "metadata": {},
   "source": [
    "It is very clear growth from our annual trending chart in total sales across the stores. From this trend, we are able to expect our warehouse capacity to increase, and We might expect continued growth in few days, months."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db584e4c",
   "metadata": {},
   "source": [
    "##### Plotting Seasonality trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "SELECT\n",
    "    TRUNC(date, 'MM') as month,\n",
    "    SUM(sales) as sales\n",
    "FROM train\n",
    "GROUP BY TRUNC(date, 'MM')\n",
    "ORDER BY month;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect data\n",
    "query_seasonality_trending =\"\\\n",
    "    SELECT TRUNC(date, 'MM') as month,\\\n",
    "            sum(sales) as sales \\\n",
    "    FROM train \\\n",
    "    GROUP BY TRUNC(date, 'MM') \\\n",
    "    ORDER BY month;\"\n",
    "collected_seasonality_trending = spark.sql(query_seasonality_trending).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_seasonality_trending[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de927a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dataframe for plotting\n",
    "df_seasonality_trending = pd.DataFrame(collected_seasonality_trending, columns=['month', 'thousand_sales'])\n",
    "df_seasonality_trending['thousand_sales'] = df_seasonality_trending['thousand_sales'].apply(lambda x: x/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae195bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "##\n",
    "plt.plot(df_seasonality_trending['month'], df_seasonality_trending['thousand_sales'], color='red', marker='o')\n",
    "plt.title('Seasonality trending', fontsize=13)\n",
    "plt.xlabel('monthly', fontsize=13)\n",
    "plt.ylabel('sales in thousand', fontsize=13)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seasonality trending in each year\n",
    "#libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import datetime\n",
    "\n",
    "df_plotting = df_seasonality_trending.copy() #stop cause error when editing on same df\n",
    "df_plotting['month'] = pd.to_datetime(df_plotting['month'])\n",
    "df_plotting.set_index('month', inplace=True)\n",
    "\n",
    "pt_df_plotting = pd.pivot_table(df_plotting,\n",
    "                    index=df_plotting.index.month,\n",
    "                    columns=df_plotting.index.year)\n",
    "pt_df_plotting.columns = pt_df_plotting.columns.droplevel() # remove the double header (0) as pivot creates a multiindex.\n",
    "\n",
    "ax = plt.figure(figsize=(9.5,5)).add_subplot()\n",
    "ax.plot(pt_df_plotting, marker='o')\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel('thousand sales')\n",
    "\n",
    "ticklabels = [datetime.date(2023, item, 1).strftime('%b') for item in pt_df_plotting.index] #get month label in string format\n",
    "ax.set_xticks(np.arange(1,13))\n",
    "ax.set_xticklabels(ticklabels) #add monthlabels to the xaxis\n",
    "ax.set_title('Seasonality trending monthly')\n",
    "#ax.xaxis(\"New\")\n",
    "ax.legend(pt_df_plotting.columns.tolist(), loc='center left', bbox_to_anchor=(0.85, .85)) #add the column names as legend.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a4085",
   "metadata": {},
   "source": [
    "We sees that monthly parttern is increasing from Feb, peak on Jun and start to going down on Dec. This pattern is stable from year 2013 to year 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a34c818",
   "metadata": {},
   "source": [
    "##### Plotting the weakly seasonality trending\n",
    "\n",
    "0 = Monday, 1 = Tuesday, 2 = Wednesday, 3 = Thursday, 4 = Friday, 5 = Saturday, 6 = Sunday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "SELECT YEAR(date) as year, WEEKDAY(date) as weekday, AVG(sales) as avg_sales\n",
    "FROM (SELECT date, SUM(sales) as sales\n",
    "      FROM train\n",
    "      GROUP BY date\n",
    "      ORDER BY date)\n",
    "GROUP BY year, weekday\n",
    "ORDER BY year, weekday;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2046bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect data to convert to dataframe\n",
    "query_weekday_seasonality_trending ='''\\\n",
    "SELECT YEAR(date) as year, WEEKDAY(date) as weekday, AVG(sales) as avg_sales \\\n",
    "FROM (SELECT date, SUM(sales) as sales \\\n",
    "        FROM train \\\n",
    "        GROUP BY date) \\\n",
    "GROUP BY year, weekday \\\n",
    "ORDER BY year, weekday;'''\n",
    "\n",
    "#collected_weekday_seasonality_trending = spark.sql(query_weekday_seasonality_trending).toPandas()\n",
    "\n",
    "#map weekday\n",
    "map_weekday = {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\",\n",
    "               3: \"Thursday\", 4: \"Friday\", 5: \"Saturday\", 6: \"Sunday\"}\n",
    "#convert to dataframe for plotting\n",
    "df_weekday_seasonality_trending = spark.sql(query_weekday_seasonality_trending).toPandas()\n",
    "df_weekday_seasonality_trending['weekday'] = df_weekday_seasonality_trending['weekday'].map(map_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekday_seasonality_trending.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weekday seasonality trending in each year\n",
    "#libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import datetime\n",
    "\n",
    "df_plotting = df_weekday_seasonality_trending.copy() #stop cause error when editing on same df\n",
    "#df_plotting['month'] = pd.to_datetime(df_plotting['month'])\n",
    "df_plotting.set_index('weekday', inplace=True)\n",
    "\n",
    "pt_df_plotting = pd.pivot_table(df_plotting,\n",
    "                    index=df_plotting.index,\n",
    "                    columns=df_plotting.year, sort=False)\n",
    "pt_df_plotting.columns = pt_df_plotting.columns.droplevel() # remove the double header (0) as pivot creates a multiindex.\n",
    "\n",
    "#sorting Indext to start with sunday and end in Saturday\n",
    "pt_df_plotting = pd.concat([pt_df_plotting.loc[['Sunday']], pt_df_plotting.loc['Monday': 'Saturday']], axis=0)\n",
    "\n",
    "ax = plt.figure(figsize=(9.5,5)).add_subplot()\n",
    "ax.plot(pt_df_plotting, marker='o')\n",
    "ax.set_xlabel(\"weekday\")\n",
    "ax.set_ylabel('average thousand sales')\n",
    "\n",
    "ax.set_title('Weekday seasonality trending')\n",
    "#ax.xaxis(\"New\")\n",
    "ax.legend(pt_df_plotting.columns.tolist(), loc='center left', bbox_to_anchor=(0.15, .85)) #add the column names as legend.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9ae55",
   "metadata": {},
   "source": [
    "The basic weekday pattern with our data is an increasing consumption start on Monday, peak at Sunday and dropping at Monday again. We can predict on this trending for weekday consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bae88",
   "metadata": {},
   "source": [
    "### Build a Forecast\n",
    "We will start to build a single forecast with FBProphet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8406e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "SELECT CAST(date as date) as ds, store, item, sales as y\n",
    "FROM train\n",
    "WHERE store=1 AND item=1\n",
    "ORDER BY ds;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e81a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query to collect data\n",
    "query_1store_forecast = \"\"\"\n",
    "SELECT CAST(date as date) as ds, sales as y\n",
    "FROM train\n",
    "WHERE store=1 AND item=1\n",
    "ORDER BY ds;\n",
    "\"\"\"\n",
    "#assemble dataset in pandas df\n",
    "df_1store_forecast = spark.sql(query_1store_forecast).toPandas()\n",
    "\n",
    "#drop any missing records\n",
    "df_1store_forecast = df_1store_forecast.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8784e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1store_forecast.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b61724",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1store_forecast.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8ba38",
   "metadata": {},
   "source": [
    "We now will import fbprophet library, and tune the logging setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb680b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "import logging\n",
    "\n",
    "#disable information messages from fbprophet\n",
    "logging.getLogger('py4j').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3688fd17",
   "metadata": {},
   "source": [
    "We will set overall growth param to linear base on trending EDA above and enable the evaluation of weekly and uearly seasonal patterns. We might also wish to set our seasonality mode to multiplicative as the seasonal pattern seems to grow with overall growth is sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417eea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model params\n",
    "model = Prophet(\n",
    "    interval_width=0.95,\n",
    "    growth='linear',\n",
    "    daily_seasonality=False,\n",
    "    weekly_seasonality=True,\n",
    "    yearly_seasonality=True,\n",
    "    seasonality_mode='multiplicative'\n",
    ")\n",
    "#fit the model to historical data\n",
    "model.fit(df_1store_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a8eef",
   "metadata": {},
   "source": [
    "We completed the trained model, lets use it for 90-day forrecast on first single store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d22333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define dataset, include historical dates with 90 day forecast\n",
    "datedt_hist_fure = model.make_future_dataframe(\n",
    "            periods=90,\n",
    "            freq='d',\n",
    "            include_history=True\n",
    ")\n",
    "df_forecast_hist_fure = model.predict(datedt_hist_fure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "datedt_hist_fure.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_hist_fure.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_fig = model.plot_components(df_forecast_hist_fure)\n",
    "display(trends_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dc9c8f",
   "metadata": {},
   "source": [
    "#### Evaluate our model using Visualize function with model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f5990",
   "metadata": {},
   "source": [
    "We now will see how our actual and predicted data line ip as well as a forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fig = model.plot(df_forecast_hist_fure, xlabel='date', ylabel='sales')\n",
    "\n",
    "#adjust figure to display dates from last year and the 90 day forecast\n",
    "xlim = predict_fig.axes[0].get_xlim()\n",
    "new_xlim = (xlim[1]-(180.0+365.0), xlim[1]-90.0)\n",
    "predict_fig.axes[0].set_xlim(new_xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae777f4",
   "metadata": {},
   "source": [
    "The black dot represents our actual data, and the blue line represents our predicted data, the light blue band represents our (95%) uncertainty interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dbe9db",
   "metadata": {},
   "source": [
    "#### Evaluate model with MAE, MSE, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(\"2018-1-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "from datetime import date\n",
    "\n",
    "#get historical actuals & predictions for comparison pandas.to_datetime(data['date'])\n",
    "df_actuals_data = df_1store_forecast[df_1store_forecast['ds'] < pd.Timestamp(2018, 1, 1)]['y']\n",
    "df_predicted_data = df_forecast_hist_fure[df_forecast_hist_fure['ds'] < pd.Timestamp(2018, 1, 1)]['yhat']\n",
    "\n",
    "#calculate evaluation metrics\n",
    "mae = mean_absolute_error(df_actuals_data, df_predicted_data)\n",
    "mse = mean_squared_error(df_actuals_data, df_predicted_data)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "# print metrics to the screen\n",
    "print( '\\n'.join(['MAE: {0}', 'MSE: {1}', 'RMSE: {2}']).format(mae, mse, rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95e950",
   "metadata": {},
   "source": [
    "#### Evaluate using Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02805a35",
   "metadata": {},
   "source": [
    "selecting cutoff points in the history, and for each of them fitting the model using data only up to that cutoff point. We can then compare the forecasted values to the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff425f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cross validation from Fprophet with argument below\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "#initial = size of train dataset, with 3 years training data over our 5 year dataset\n",
    "#horizon = size to be forecasted, 90 days forecast\n",
    "#period = spacing between cutoff dates as incremental, ussually halft of horizon, every 45 days\n",
    "df_cv_1store = cross_validation(model, initial='1065 days', period='90 days', horizon='90 days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_1store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df452c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The performance_metrics utility can be used to compute some useful statistics of the prediction performance\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "df_p = performance_metrics(df_cv_1store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b84053",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "##visualized with plot_cross_validation_metric\n",
    "from fbprophet.plot import plot_cross_validation_metric\n",
    "fig = plot_cross_validation_metric(df_cv_1store, metric='mape')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dad42f",
   "metadata": {},
   "source": [
    "### Scaling Model Training & Forecasting\n",
    "Now we going to build numberous, fine-grain models and forecast for indivual store and item combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fddcf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##sql to collect data from database\n",
    "sql_all_store_items = \"\"\"\n",
    "                    SELECT store, item, DATE(date) as ds, SUM(sales) as y\n",
    "                    FROM train\n",
    "                    GROUP BY store, item, ds\n",
    "                    ORDER BY store, item, ds\n",
    "                    \"\"\"\n",
    "\n",
    "spsql_store_item_history = (\n",
    "    spark.sql(sql_all_store_items).repartition(sc.defaultParallelism, ['store', 'item'])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb2a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spsql_store_item_history.show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba1613",
   "metadata": {},
   "source": [
    "Our goal is to build a model for each store and item combination, so we will need to pass in a store-item subset from the dataset we assembled, train a model on that subset, and receive a store-item forecast back. We'd expect that forecast to be returned as a datset with a structure like this where we retrain the store and item identifiers for which the forecast was assembled and we limit the output ot just the relevant subset of fields generated by the Prophet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1727348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "#create schema for the forecast output returned\n",
    "result_schema = StructType([\n",
    "    StructField('ds', DateType()),\n",
    "    StructField('store', IntegerType()),\n",
    "    StructField('item', IntegerType()),\n",
    "    StructField('y', FloatType()),\n",
    "    StructField('yhat', FloatType()),\n",
    "    StructField('yhat_upper', FloatType()),\n",
    "    StructField('yhat_lower', FloatType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed8f92",
   "metadata": {},
   "source": [
    "Using Pandas user-defined function (UDF) to train the model and generate a forecast. We will define this function to receive a subset of data organized around a store and item combination. It will return a forecast in the format identified in the schema structure we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae110a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)\n",
    "def forecast_store_item(history_pd):\n",
    "    #TRAIN MODEL\n",
    "    # -------------------------\n",
    "    #remove missing values\n",
    "    history_pd = history_pd.dropna()\n",
    "    \n",
    "    # configure the model\n",
    "    model = Prophet(\n",
    "        interval_width=0.95,\n",
    "        growth='linear',\n",
    "        daily_seasonality=False,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        seasonality_mode='multiplicative'\n",
    "    )\n",
    "    \n",
    "    #train the model\n",
    "    model.fit(history_pd)\n",
    "    #--------------------\n",
    "    \n",
    "    #BUILD FORECAST\n",
    "    #--------------------\n",
    "    #make predictions\n",
    "    furture_pd = model.make_future_dataframe(\n",
    "        periods=90,\n",
    "        fred='d',\n",
    "        include_history=True\n",
    "    )\n",
    "    \n",
    "    forecast_pd = model.predict(furture_pd)\n",
    "    #--------------------\n",
    "    \n",
    "    #ASSEMBLE EXPECTED RESULT SET\n",
    "    #--------------------\n",
    "    # get relevant fields from forecast\n",
    "    f_pd = forecast_pd[['ds', 'yhat', 'yhat_upper', 'yhat_lower']].set_index('ds')\n",
    "    \n",
    "    #get relevant fields from history\n",
    "    h_pd = history_pd[['ds', 'store', 'item', 'y']].set_index('ds')\n",
    "    \n",
    "    #Join history and forecast\n",
    "    results_pd = f_pd.join(h_pd, how='left')\n",
    "    results_pd.reset_index(level=0, inplace=True)\n",
    "    \n",
    "    #get store & item from incoming data set\n",
    "    results_pd['store'] = history_pd['store'].iloc[0]\n",
    "    results_pd['item'] = history_pd['item'].iloc[0]\n",
    "    #-------------------\n",
    "    \n",
    "    #return expected dataset\n",
    "    return results_pd[['ds', 'store', 'item', 'y', 'yhat', 'yhat_upper', 'yhat_lower']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782ece3",
   "metadata": {},
   "source": [
    "call our UDF to build our forecasts. we do this by grouping our historical dataset around store and item. We then apply our UDF to each group and tack on today's date as our training_date for data management purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52cc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spsql_store_item_history.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66110fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "#Group data on store and item level, then use function forecast to forecast on thi store and item level\n",
    "results = (\n",
    "    spsql_store_item_history.groupBy('store',\n",
    "        'item').apply(forecast_store_item).withColumn('training_date', current_date())\n",
    ")\n",
    "\n",
    "results.createOrReplaceTempView('new_forecasts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef4f63",
   "metadata": {},
   "source": [
    "save our results to a queriable table structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9652b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark --packages io.delta:delta-core_2.12:1.0.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd2203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "create table if not exists forecasts (\n",
    "  date date,\n",
    "  store integer,\n",
    "  item integer,\n",
    "  sales float,\n",
    "  sales_predicted float,\n",
    "  sales_predicted_upper float,\n",
    "  sales_predicted_lower float,\n",
    "  training_date date\n",
    "  )\n",
    "using delta\n",
    "partitioned by (training_date);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23956e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "-- create forecast table\n",
    "CREATE TABLE IF NOT EXISTS forecasts (\n",
    "    date date,\n",
    "    store integer,\n",
    "    item integer,\n",
    "    sales float,\n",
    "    sales_predicted float,\n",
    "    sales_predicted_upper float,\n",
    "    sales_predicted_lower float,\n",
    "    training_date date\n",
    ")\n",
    "USING delta\n",
    "PARTITIONED BY (training_date);\n",
    "\n",
    "-- load data to it\n",
    "INSERT INTO forecasts\n",
    "SELECT ds as date,\n",
    "    store,\n",
    "    store,\n",
    "    item,\n",
    "    y as sales,\n",
    "    yhat as sales_predicted,\n",
    "    yhat_upper as sales_predicted_upper,\n",
    "    yhat_lower as sales_predicted_lower,\n",
    "    training_date\n",
    "FROM new_forecasts;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f92b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "lighthouse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
