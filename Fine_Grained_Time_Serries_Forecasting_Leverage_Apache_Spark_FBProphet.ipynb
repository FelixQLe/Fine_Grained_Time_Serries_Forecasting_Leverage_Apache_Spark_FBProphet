{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca7abeb9",
   "metadata": {},
   "source": [
    "- Intructions on this Repo\n",
    "1. Heap size error run this command line before run jupyter notebook on command line\n",
    "\n",
    "export PYSPARK_SUBMIT_ARGS=' --conf spark.sql.shuffle.partitions=700 --conf spark.default.parallelism=700 --driver-memory 30g --driver-cores 6 --executor-memory 30g --executor-cores 6 pyspark-shell'\n",
    "\n",
    "2. ERROR PythonRunner: Python worker exited unexpectedly (crashed) java.net.SocketException: Connection reset\n",
    "\n",
    "Try to run several times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bedb0a",
   "metadata": {},
   "source": [
    "## Demand Forecasting\n",
    "\n",
    "The objective of this notebook is to illuate how we might leverage the Apache Spark - the efficient distribution of the work required to generate hundreds of thousands or even millions of ML models in a timely manner and FBProphet - popular library for demanding forecasting.\n",
    "\n",
    "NOTE: The original notebook utilized the DataBricks and DataLakes. In this notebook, I will use my Spark SQL and localhost storage to emulate the orginal notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f447bf",
   "metadata": {},
   "source": [
    "### Exploring Data\n",
    "- Source: https://www.kaggle.com/competitions/demand-forecasting-kernels-only/data\n",
    "\n",
    "A relatively simple and clean dataset, given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores. \n",
    "\n",
    "What's the best way to deal with seasonality? Should stores be modeled separately, or can you pool them together? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36f3bb",
   "metadata": {},
   "source": [
    "#### Import Data from local to sql database\n",
    "- use pyspark to import data to sql database, to work similarly to DataBricks notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b15375fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#read data\n",
    "data_path = pd.read_csv('./Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SQL databse using Pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0426b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b402c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import spark\n",
    "\n",
    "#structure of the training dataset\n",
    "train_schema = StructType([\n",
    "    StructField('date', DataType()),\n",
    "    StructField('store', IntegerType()),\n",
    "    StructField('item', IntegerType()),\n",
    "    StructField('sales', IntegerType())\n",
    "])\n",
    "\n",
    "#read the training data into spark dataframe\n",
    "train = spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a788662e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "lighthouse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
